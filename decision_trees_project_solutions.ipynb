{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision trees project_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themikepeng/projecttitanicdecisiontrees/blob/main/decision_trees_project_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7cCNvx2U-v"
      },
      "source": [
        "Project Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h2p12912aix"
      },
      "source": [
        "#Project Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Tjiw16064a"
      },
      "source": [
        "##Drive Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxACAYud1Eej"
      },
      "source": [
        "#Drive Setup\n",
        "Ensure that this notebook and all data files are saved under the `Colab Notebooks/decision trees/` folder!\n",
        "\n",
        "You must follow the steps to mount your Google Drive so that your data file(s) can be used:\n",
        "\n",
        "\n",
        "*   Run the cell below\n",
        "*   Go to the URL given and sign into your Google account\n",
        "*   Copy and enter the authorization code into the textbox under the cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6xplFrp0qU1",
        "outputId": "5d438220-d42d-4cfd-c72c-44f25b8b815e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1oL7IF9-rme"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L5u5t_ATBi6",
        "outputId": "5c4437b7-e68b-4065-cf34-5aab7811f2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pydot\n",
        "\n",
        "import io\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import sklearn.model_selection\n",
        "import sklearn.tree\n",
        "from numpy import genfromtxt\n",
        "from scipy import stats\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "import pydot\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "eps = 1e-5  # a small number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLtmZfZKJAB"
      },
      "source": [
        "# Intro to the Titanic dataset\n",
        "\n",
        "**Goal: we want to implement a decision tree model that can predict whether a passenger on the Titanic survived or didn't survive**\n",
        "*  The Titanic dataset is split between training data and test data\n",
        "  *  The training data will then be further split into training and validation sets\n",
        "  *  The test data does not contain the true class labels: we don't know if these passengers survived or not\n",
        "  *  It is good practice to not predict on the test set until we've finalized our model!\n",
        "*  We will then train an instance of this decision tree on the training set (sample points), and ensure that it is generalizable to the validation set\n",
        "*  Finally, we will make our predictions on the test set\n",
        "\n",
        "*  The Titanic dataset has the following fields:\n",
        "\n",
        "1. survived: the label we want to predict; 1 indicates the person survived, whereas 0 indicates the person\n",
        "died\n",
        "2. pclass: Measure of socioeconomic status; 1 is upper, 2 is middle, 3 is lower\n",
        "4. sex: Male/female\n",
        "3. age: Fractional if less than 1\n",
        "5. sibsp: Number of siblings/spouses aboard the Titanic\n",
        "6. parch: Number of parents/children aboard the Titanic\n",
        "7. ticket: Ticket number\n",
        "8. fare: Fare\n",
        "9. cabin: Cabin number\n",
        "10. embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
        "\n",
        "Let's start with some warmup exercises, which will go over Python concepts that are useful for our implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5zBgF3DJF1Z"
      },
      "source": [
        "# Warmup A: Break, Continue\n",
        "\n",
        "In a `for` or `while` loop:\n",
        "*   The `break` statement terminates the loop that contains it\n",
        "*   The `continue` statement skips the rest of the code for the current iteration, and continues onto the next iteration\n",
        "\n",
        "Note: if used in a nested loop, `break` and `continue` only affect the innermost loop!\n",
        "\n",
        "Examples: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t48UHSAvXTMY",
        "outputId": "beb4d056-dd45-4b3e-ba65-effce86998b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "hello = \"Hello World\"\n",
        "\n",
        "print(\"First word:\")\n",
        "for c in hello:\n",
        "    if c == \" \":\n",
        "        break\n",
        "    print(c)\n",
        "\n",
        "print(\"\\nNo vowels:\")\n",
        "for c in hello:\n",
        "    if c in ['a', 'e', 'i', 'o', 'u']:\n",
        "        continue\n",
        "    print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word:\n",
            "H\n",
            "e\n",
            "l\n",
            "l\n",
            "o\n",
            "\n",
            "No vowels:\n",
            "H\n",
            "l\n",
            "l\n",
            " \n",
            "W\n",
            "r\n",
            "l\n",
            "d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBJjTeyREnit"
      },
      "source": [
        "## Try it\n",
        "Using `break` and `continue`, write a function `wacky_sum` that scans an array of integers from left to right, taking a running total of elements that are **not** a multiple of 10, and stopping when encountering a negative element"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BciAzeQN7nfZ",
        "outputId": "02a1e60f-2dda-43d1-c168-2824cdb61275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def wacky_sum(nums):\n",
        "    tot = 0\n",
        "    for num in nums:\n",
        "        if num < 0:\n",
        "            break\n",
        "        if num % 10 == 0:\n",
        "            continue\n",
        "        tot += num\n",
        "    return tot\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=6)\"\n",
        "import doctest\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(wacky_sum([]))\n",
        "  0\n",
        "  >>> print(wacky_sum([-1]))\n",
        "  0\n",
        "  >>> print(wacky_sum([200]))\n",
        "  0\n",
        "  >>> print(wacky_sum([8, 7, 7, 5, 10, 6, -8, 5]))\n",
        "  33\n",
        "  >>> print(wacky_sum([0, 5, 10, 15, 20, 25, 30, -35, 40, -45, 50]))\n",
        "  45\n",
        "  >>> print(wacky_sum([2017, 2018, 2019, -2020, 2021, -2022]))\n",
        "  6054\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP5qbQyVCYR1"
      },
      "source": [
        "# Warmup B: List Comprehension\n",
        "A list comprehension is a way to define a new list or 2D array based on an existing list, as a more elegant alternative to using an explicit `for` loop\n",
        "\n",
        "A traditional `for` loop would have the format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvlDGT3Hr2r",
        "outputId": "4d80000c-0daf-45ee-c235-5ad16f0450cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "list2 = []\n",
        "for (item) in list1:\n",
        "    if (conditional):\n",
        "        list2.append((operation))\n",
        "return list2\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlist2 = []\\nfor (item) in list1:\\n    if (conditional):\\n        list2.append((operation))\\nreturn list2\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0Aki-oH4zB"
      },
      "source": [
        "While a list comprehension would have the format\n",
        "\n",
        "`list2 = [(operation) for (item) in list1 if (conditional)]`\n",
        "\n",
        "Note that the if conditional is optional!\n",
        "\n",
        "For example, if we wanted to find the lengths of all the elements in a list that start with 'a', we could use the following list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ExNc6hMhlh",
        "outputId": "f6f46930-4506-48bf-9da5-ea4b8a4b76c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "animals = ['anteater', 'quail', 'moose', 'ape', 'dolphin']\n",
        "lens = [len(str) for str in animals if str[0] == 'a']\n",
        "lens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJdzqYCRQhpK"
      },
      "source": [
        "### Try it\n",
        "Write a function `apply_evenly()` that takes in a function `func` and applies it to every other element (all even-indexed elements) in the list `list1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t7egvJnUYFc",
        "outputId": "68d62136-67bd-4f6e-8c28-7e036abb36fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def apply_evenly(func, list1):\n",
        "    return [func(list1[i]) for i in range(0, len(list1)) if i % 2 == 0]\n",
        "\n",
        "    #alternative solution: return [func(list1[i]) for i in range(0, len(list1), 2)]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=5)\"\n",
        "import doctest\n",
        "\n",
        "rand = [8, 5, 5, 1, 2, 3]\n",
        "langs = ['Python', 'Java', 'C', 'C++', 'HTML', 'Javascript']\n",
        "\n",
        "def lower2(str):\n",
        "    return str.lower()\n",
        "\n",
        "def sq(x):\n",
        "    return x ** 2\n",
        "\n",
        "def spread4(x):\n",
        "    return (np.ones(4) * x).tolist()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(apply_evenly(len, langs))\n",
        "  [6, 1, 4]\n",
        "  >>> print(apply_evenly(len, animals))\n",
        "  [8, 5, 7]\n",
        "  >>> print(apply_evenly(lower2, langs))\n",
        "  ['python', 'c', 'html']\n",
        "  >>> print(apply_evenly(sq, rand))\n",
        "  [64, 25, 4]\n",
        "  >>> print(np.array(apply_evenly(spread4, rand)))\n",
        "  [[8. 8. 8. 8.]\n",
        "   [5. 5. 5. 5.]\n",
        "   [2. 2. 2. 2.]]\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJuinCUeuB5s"
      },
      "source": [
        "# Warmup C: Multidimensional Select\n",
        "\n",
        "Suppose we have a data matrix `wellness` that contains results of a wellness survey on students, with the following feature labels (in order):\n",
        "*  name\n",
        "*  sleep: the hours of sleep the student got the previous night\n",
        "*  meals: how many meals the student ate the previous day\n",
        "*  fruit: whether the student ate any fruit the previous day (1 for \"yes\", 0 for \"no\")\n",
        "\n",
        "Note that each row represents one student (sample point), and each column represents a feature!\n",
        "\n",
        "We can use the row and column index to select or assign elements in a 2D Numpy ndarray (eg. a data matrix):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yybDj3ooETYD",
        "outputId": "4998b2dc-cc5e-4136-a6b2-41de4fbee08f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "wellness = np.array([[\"Alice\", 9, 3, 1], [\"Bob\", 7, 3, 0], [\"Sam\", 8, 3, 1], [\"Tina\", 6, 2, 1]])\n",
        "feature_labels = [\"name\", \"sleep\", \"meals\", \"fruit\"]\n",
        "print(\"Our ndarray: \\n\", wellness)\n",
        "\n",
        "#select from row 0, column 3: \n",
        "elem = wellness[0, 3]\n",
        "print(\"\\nDid Alice have fruit yesterday?\\n\", elem)\n",
        "\n",
        "#update row 2, column 1\n",
        "wellness[2, 1] = 9\n",
        "print(\"\\nSam says he actually slept 9 hours yesterday: \\n\", wellness)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our ndarray: \n",
            " [['Alice' '9' '3' '1']\n",
            " ['Bob' '7' '3' '0']\n",
            " ['Sam' '8' '3' '1']\n",
            " ['Tina' '6' '2' '1']]\n",
            "\n",
            "Did Alice have fruit yesterday?\n",
            " 1\n",
            "\n",
            "Sam says he actually slept 9 hours yesterday: \n",
            " [['Alice' '9' '3' '1']\n",
            " ['Bob' '7' '3' '0']\n",
            " ['Sam' '9' '3' '1']\n",
            " ['Tina' '6' '2' '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIdzEjnMIvvd"
      },
      "source": [
        "We can also select an entire row or column by index to get a 1D array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0PR4uyPMpgT",
        "outputId": "ce3876ef-cbee-4c26-9fd7-fd72d6951854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#select row 3\n",
        "row3 = wellness[3, :]\n",
        "print(\"Tina: \\n\", row3)\n",
        "\n",
        "#select column 1\n",
        "col1 = wellness[:, 1]\n",
        "print(\"sleep: \\n\", col1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tina: \n",
            " ['Tina' '6' '2' '1']\n",
            "sleep: \n",
            " ['9' '7' '9' '6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmtEws6IOpch"
      },
      "source": [
        "Using a condition in the select statement will give us a filtered 2D array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR1rtCnkVxxB",
        "outputId": "33bfd526-8fc6-434a-9a44-dca69607dec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#select rows where column 1 is less than '8'\n",
        "lessthan8 = wellness[wellness[:, 1] < '8']\n",
        "print(\"Less than 8 hours of sleep: \\n\", lessthan8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Less than 8 hours of sleep: \n",
            " [['Bob' '7' '3' '0']\n",
            " ['Tina' '6' '2' '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiG9lSpMdu0e"
      },
      "source": [
        "The `shape()` function of a ndarray gives us a tuple of array dimensions! Thus, for a 2D array:\n",
        "*  `shape[0]` gives us the number of rows (sample points)\n",
        "*  `shape[1]` gives us the number of columns (features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OjxfX-KemBK",
        "outputId": "86a72028-0ae1-478b-f7fe-5d69a0913f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"Number of students:\", wellness.shape[0])\n",
        "print(\"Number of features:\", wellness.shape[1])\n",
        "print(\"\\n\")\n",
        "\n",
        "#print all names\n",
        "for i in range(0, wellness.shape[0]):\n",
        "    print(wellness[i, 0])\n",
        "print(\"\\n\")\n",
        "\n",
        "#print all feature labels\n",
        "for j in range(0, wellness.shape[1]):\n",
        "    print(feature_labels[j])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of students: 4\n",
            "Number of features: 4\n",
            "\n",
            "\n",
            "Alice\n",
            "Bob\n",
            "Sam\n",
            "Tina\n",
            "\n",
            "\n",
            "name\n",
            "sleep\n",
            "meals\n",
            "fruit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoOuy_A5hbdf"
      },
      "source": [
        "### Try it\n",
        "Use a select statement to find the names of all the students who had at least 3 meals yesterday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33o4Wtjo-6q4",
        "outputId": "58fdec68-ec36-4c2a-c628-e916712ba47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "names = ...\n",
        "names = wellness[wellness[:, 2] >= '3'][:, 0]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=1)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(names)\n",
        "  ['Alice' 'Bob' 'Sam']\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vchq93XqT6La"
      },
      "source": [
        "# Warmup D: Information Gain Calculation\n",
        "\n",
        "Recall the equations for calculating information gain:\n",
        "\n",
        "![entropy after split.png](data:image/png;base64,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)\n",
        "\n",
        "![information gain equation.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUYAAAApCAIAAADGYBecAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABDMSURBVHhe7Zx/TBRZnsBrl+0JKUOu11yJS8ezdS2cWCHHmOBs7DGCcdpETBbN2bPAZLrlXDzEmQ4bfhzgyLLXYGTMkb6ABtaFJju4ERNhdmkz1JCx3Z0mp1y0T6eMUo6UyxQRKtG+EHuNdX3ee1Wvm+pfNA0Nw2B9/uiq97p+vO+r933f7/e9V/WDV69eYSoqKquFH6KtiorKqkBVaRWVVYWq0ioqqwpVpVVUVhWqSquorCpUlVZRWVWoKq2isqpQVVpFZVUxh0ozjiKAg0HJRJhhBs5WHTXD84tKqpyPUfZKZ4aj7QMKeQX6FBCggRZQ+vuIMNQAZRhaOTIwDtgwzI57KB2B4KyFDadh2IsyFAh/aT1ubqCnUBLi9/EjA+026/ESeBbAfNza1DbgnvChAxDS06xx8iiZTLzDsJKLGukoJZbxdEolW5A2JchSWGneaWvqu8X7MJxIJ4g0vXYt+mNlwzhO1DluxHwoKslhihsTwYYiN8jpCF5wHLQBeOZGrZwRxHfX0XDeQx2vMaajHMzH9p4sq2rrc98TvC9QnugVmJG+9poya8eo148yMYx4y0BiE/2uByidRLhvWPCLk5vCSxyAH5d0mSI3ScklZQ6VpiwXARYKJeePV5gAv1Sp/YK91W63lxvS5PwVD2xqSgjjb0ANNBoJlP4+QuxrhDLsWykyiBzHgU16pj5WqxD4Mbh5KzNM5wX6zFn6+c4TR97GUQ7mdbfZnI9FDZFTXPvJhR7YXgE99ubyg9mERhSut7YOz7onxOZMHPNdG4Xql1R47j7cvPVTvZSMRJzkYTEIUh8s+tKxdLG0jojVZam8xnDf3IabN/U6KRmJjxuDzX+jXp8qZ0j4BfpcL4tll76fM6sVT2/SHtANU0caK/KzdLgGZWsIveFw9enjBpDBXqFnNXgztQNc/y83k6zTM9wYDAT0en2gBOFwY3fhhtoUS+hk8iO0jQKIpZtozFiPDPVsct2tvu5LNDPhEzGNdpvBVPR+7ma5nuVjZOimIrhLmu3IRICY58Zg71UX88gLnoNGq6fyTEcOZRMp8E8ZpquoaRgz1l4w3D9zZpAFrrsuy1RuNT7/vZx/0bLe09vRSd8DV9DgG3aYjh0xglv7vZ4rnb3DDD8jatJ01H7LiQMUrrgsNsO7vxxwfXl7TABlBvfGCXJH/s8PGrOQ7QIBp7VHftBysUmLHRhnEH1ZHQ/lfelPCd+Ee/CS03WPg55eqla/LddkNmUrraBAN1gd7BaLveGd51992nvJzUjl1WUZi8OOjIHvkevTi8CZlMWkjIWlJt1NdM3fKMoSTy6ALFrwKcwmdz53XertG2GAFLDS9haHPYulwTsOniqo35gGDRt/BH1UzZv6kBodanM8EEnzEYPSTshOdTpFRjMeOHgu+9YImMIwpuh0GzHs8dj4U4xMYjA4wUolztQHw4Ewno6PzYANqf8HOb20JGqln432WCvPDniEN7TphDZV9N5zdZ4sax2RQ9A3cBA8p2ulzkoDDgCxtDZVSqGYZ8Dz6LmGgMdgXs7T32L9ZVPIUIfE+JUztn4O0xLEGnH6/9bIlwM8G3Uct7Y42ZfyrYFqOU5W9j7gaZu1pZ95lqoltBpxhvdcaqr8lJl1oqfohhNV7VCvMLlIwPsS7rkcpyuDg0YacG5IsbVvRG/cPvZiXVlN+8AtDooBigHEuDXQYjU3KRw8hMg5W8rqOlxjfnBxeFMeHFlp7YtnI4ThprKTna57XixNqqgnHnCDhj9xsxLJzEOuWICiNVnrOr8cewluAc4DldbfUvmrPnY28owC6HDj0BVv9MfPcTCUjRInBxC4+1BQarMi6vS6uy+xWJrRtDu0O0z78TrwzKboLzxhI2ESaVS+2WIxGxSWUbsOJtjJpA6RCYHBgZhx8l85+MzTMjcty6DSHFY6KqP0kNZwwl6+U6pcv8/TU9ky7B3tcbI7i0mMNLXaTchW55W3BuNw32iXHPPklp4sNcjPxc+7/qOpc5Rx/JtDd9ZCKbws9gGfW3m+dLvUvUqNTH4Eo8O0dle5HVwA6Jvf5z5f1j7ipZvrxLQd5SBily7ru9X50VmXd8jleY/Kgdfknf/uYEWMPNhcc0iPTLffx/yhoekqz150MvtgIbW7K+y7I4sdju9Gt22Qg2IcO1kq1wC4wfXWpo5RpqvBkWG3bAt0P4DHLlpDmmw1BbIL4+edtrreB8JAv/tAtWHWdIQxMdDSBfojUMmNgUoW3J0N7cMu6e8g85IrFtx1WrPV1PyrAhTQTjobantZULSRA9W7YhYtCUxyktJHxMlB0NgYkakwruzVXuBf6w6/q2wkkNQcUxF5s4elW8qYnSbLYSOVrqj/KGhw6arjU14sK1lhocg9goMDc8TJaGwsKzOmZ5JUElVpDN9fgZoaIAXPLjpiGG51z8zpzExe6xsBHRlZHNRnQIou96N6b11V3wQ9cKOA2q2o4i2mg7I+A5TWMi2/QtZnQApu2JPXOUKLoib/Q6TPAHx7Xl6ayzlze2wCyyFBF84wL7WaNMP7wXYPSMGpfzIZrra6xXFewKhgkeLAX7vihmIUzeozQLe7ov5pVdVlnv7MXbAtV9lSqJIKpM+AFF1+yUG6pk9g2HHMEEPfxNHBftB/6Q7XKyqZMJTWCxyoKJQBWaxc1BFrQJ8BGfmlBXTVZYF5NI7titkVUCUXL5ag/YWB4mTM3VriljJiQekz0B72AlgR4ANS+e9ECUSJfTWNWEdLzyg/0ts00odvNZgOHczbRmiUzSYSf7jHswhQnCxcriq6LGXEQL85ZqidXBJ1vDXvbAeKoiAVXyOXNLbPJnjc0MzuPJAX1shSdMYD2WDL3GCUnpPmp7rorfHtbFL5qIgMqdt7J3urlEQALxf8+kR5SiPdWN16rqejOOREACg22ps3Ux43VCrDgTD3D2jg3gIoxt2bDAyZgpA5YaZgDQ7TczWnMeY/YafxbljzBd3f/lBNW6RcW3Oyw4qWJhVtTsd78chxcny26vUBucT/vgn70ezcHaEFDoDr91Wc6/ikfD+Ja0TfAxB3WM2/rOu8zvmWWBYEipPjAkLtubrYJJKoSuszwgu2LmMj2ovF9BPomZBklF4KX6+DVuwbThnd6NevQ3uhkLro+fNF9PlmePaWm77U3mRtvYZy5820AMXYQoaMxMqkrdNBizfGPZHTMpt0YW7LWl2caUmBH4cKvymyS9MSuph9/ALk2qALUxBt+jLMmKI4ObvsAppuiqB6NzxOOcHLeKA9J/8xM5ZbC0nTGd5vvNB1rvlEQTahAd67q6OurIUWlkGrUZycD6cKo9JaLBme2PPwyWbpJrHmB2iq4HdGfCknlwK/4LncUnXMXGQ+evRYVcPZdsdnbmZmLluZOKGuwVKAXBIFyyFXCIsdHosWJ4cSOcHLMrfAL565eR4mLkWr32mqtvfYK43Aeoh3HS1/DBsH8/JS8LIpwi7NwvYeRcIEqXNGjOAGiRsnx5+HTzbftUp7BVjr6droXtXi8Qu0rbKl38OLa/Tbc43vlVZXNn7S0XOxqzoPHZEUvAJ86sSSiRGoqCDLJFdSmRiTpqQVcXIYkRO8L7xe6NZm6tfL6QB3pcXKp6Iv1SW2W05Kk9L8n0dDdZqffAR+9URsb8/3hA8fPZ9jdgrzcSwswhxxctx5+KSzHCq9TgfDb5aNmIYBDXWSgzX4E+LHcjrZ+Ea6HQ9EbK2xvu1cc2Wp5ee52dtJXZoG8yfuF6zPgGI8ZLlIU+zlOdjyFr26BnjmsGnAwa0wwppaMuWaN3B4bG5K5hhlx7yPx6AIijg5nMgJ3pln03CzThtm4uRnMSXEWr6L6yUVeuoNOUBeiwqu//coIxJ8VzWSJUjPHHMH44ESxzL78efhk85yqDSRtQPW78jgtbCW6uddV2GVxImUFsE464Gb7TlU6A18/+Wee8g1CgS1A4ZD7sHr4QrHX3NCMbZkZy7SuUrJpH4GdJr94qswj1H46gtJkADJlGu5iLsQOuYE78YIs0psglU9Q9Mj4TZVwsc4ncDd1fyMykQ5EO99D8qM1ackStz16vHn4ZPPsjjeGXmmnbCl9to63UF1kOal4cTM2oglBMlDdhCwGy73UykN8IvCLYftHBxGjcYYFzNw0uUdktYYXrR1jsxqNZyXvgw0UGv8RfiIfuJocg4cBN0ff7mpPXgLv+DusPU+RCmZxOX6zom/EDrmBK8m0qsljYdJEC+728rqPnVzgg+N1ftF3yTjtNe2DHsxDVl8KEdxptczAjpe3LhHmbko4sfJcefhl4CE56UXBJ5TcjKftzkfu9qt7m5Cu+aHoncKLgvFUinLxyHrTJILsctk/FMT/dTdfuKmdF/s+VO4eFKTlW/8m5N+yE0CxUGKuC5jC/Crud7a4/Tf6fKr6o0RIR/+9pGTB3jbIOdqs7p7CC2Oif8jvwCkoUoaQ9aZLJgNBdUlTGUX426z3pRuIRVYQ2WRzF0W06Ox8ETkWhksZiH040ngfodJQ+TVVPC1rUMCd7W97irKnCVVX1CjeGELMOl2gutvOJgbMuW5KBa4Xn2JWa7hMZwstp1vPpafvXmNKAjClBfTUob3qu2/rQ+p96SDU5bTzaV7KLiAFNz36fM1ZK6l1t5TW/zuW6CRiF/dCq7PJIzl5YYNuOaFV5hi2Imo1g4ni5rP20rzt+uhGDCW01LSKGv93qQpELG3Htwid5sWm5Eqar3BYjtfsSfU90xArpXBwhZCr5VWcEZ9FCl4ttl+obXesj9bTwRe2dDgxObsfHO9/bfNpq1KYy+OftbHYxrDobxY6pc4C1yvHhtGGvSTJw4E+tRCX9R/pfJ9YPrzU4WFhR84vkbp14XpwX8Fcp8amkbpBfLX/kpQe79e7GWWljvdhR8HSqjcT5DvehJLJQQvffqotcLaF/6avo9jYdQW8jLDawGhfxMYYHYyZA1Pogh0Vz8PQut/WSFvvgesMSAwDwfn/E/T2EOHtcgxMtSA9tG/8td1JGZn/iUzfpeGn1MJm8xDqq2yMhjr/WdojX89+O1LlPPqf19++7ntA5jbdvNvKO814k43kP2jK9+iZOI8+/OZDwrLuu88R+nvmK+7Cwu778j700MfF576XDbGYD/ojMD8wDHweMUxwf2vuz8+dSqaJVet9MqCLKgwrsXEB71VZvPxCiuw2EdLzFU9jKghjJVHpHfLXjO25eRpMOE+u8AvSE3RrRcYvbnRkrVE86SJAj8WZMmS9wmdHmN5aeodm558GFwIDPbJDGl1jTDUR2+xnEAfpYHfWmLdt6FNFvjxh5jhwyh+h6rSKwycstjt1e8ZKELzfAqOwImpBLWntLnNvmIa5fKSQhn2aSNeiZkfft7Z7hB2V9esmO80QQTJW5ZoGg68uQBUdEsGGgK9O0qjdf7CbTcreeOIwIc6MOzJJLvXFD2QQNZaRUVl6fn6dyCC6g4Mciqc8Dvdhb9D2XAoFHnUSi89BHBMwAMPR7XSKirLBjMKv7cVWGEKLDMWcLAnx4MvGk7zLGmAU5FzAI6J9fKJqtIqKssGXM40DpcBARiH1cHOOtiYYbusogIPJzdkqJy9GH05MKANX1aRJ6vBMagviERVaRWVZYMwfmjBeuTQuC+j1iIt6wUox8akMbMeq/wBOarEbsECsfRprB59gnp6Eotpx38AnG+0q6Ki8v1HtdIqKqsKVaVVVFYVqkqrqKwqVJVWUVlVqCqtorKKwLD/B+Te8bZo5RgvAAAAAElFTkSuQmCC)\n",
        "\n",
        "Where:\n",
        "*  S<sub>left</sub> is the set of class labels belonging to the left child post-split, represented by `left` below\n",
        "*  S<sub>right</sub> is the set of class labels belonging to the right child post-split, represented by `right` below\n",
        "*  S is the set of all class labels belonging to the current node, represented by `y` below\n",
        "*  |S| is the number of elements in **any** set of class labels S — for our purposes, we'll use the `.size` attribute of `numpy.ndarray` (the three sets mentioned above will be represented by ndarrays)\n",
        "*  H(S) is the entropy of **any** set of class labels S — which, for now, will be found by calling the function `entropy0` on S\n",
        "\n",
        "Fill in the information gain calculation within `igcalc()`; this calculation will be used later in our `DecisionTree` implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxAy2e2OUXoY",
        "outputId": "32300a2e-9ffe-4ffb-843d-63f1aaff603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#This is actually the gini impurity function, which has similar behavior to \n",
        "#the entropy function, and is sometimes used in place of entropy in decision\n",
        "#trees; however, we will soon implement the actual entropy function\n",
        "def entropy0(labels, base=None):\n",
        "    value,counts = np.unique(labels, return_counts=True)\n",
        "    eq = 0\n",
        "    for cnt in counts:\n",
        "        p_i = cnt / labels.size\n",
        "        eq += p_i * (1 - p_i)\n",
        "    return eq\n",
        "\n",
        "def igcalc (left, right, y):\n",
        "    calc = ...\n",
        "    calc = entropy0(y) - (left.size * entropy0(left) + right.size * entropy0(right)) / (left.size + right.size)\n",
        "    return calc\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=6)\"\n",
        "import doctest\n",
        "\n",
        "left1, right1, y1 = np.array([0, 1]), np.array([0, 1]), np.array([0, 1, 0, 1])\n",
        "left2, right2, y2 = np.array([0, 0]), np.array([1, 1]), np.array([0, 1, 0, 1])\n",
        "\n",
        "left3, right3, y3 = np.array([0, 0, 1, 0]), np.array([1, 1, 1, 2]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "left4, right4, y4 = np.array([0, 0, 2]), np.array([1, 0, 1, 1, 1]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "\n",
        "left5, right5, y5 = np.array([0, 0, 0]), np.array([1, 1, 1, 1, 2]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "left6, right6, y6 = np.array([0, 0, 0, 2]), np.array([1, 1, 1, 1]), np.array([0, 0, 1, 0, 1, 1, 1, 2])\n",
        "\n",
        "\"\"\"\n",
        "  >>> print(igcalc(left1, right1, y1))\n",
        "  0.0\n",
        "  >>> print(igcalc(left2, right2, y2))\n",
        "  0.5\n",
        "  >>> print(igcalc(left3, right3, y3))\n",
        "  0.21875\n",
        "  >>> print(igcalc(left4, right4, y4))\n",
        "  0.2270833333333333\n",
        "  >>> print(igcalc(left5, right5, y5))\n",
        "  0.39375\n",
        "  >>> print(igcalc(left6, right6, y6))\n",
        "  0.40625\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz40rs9S2lVM"
      },
      "source": [
        "# Helpful Functions\n",
        "We are now ready to implement our `DecisionTree` class! Each section below will walk through the implementation of a function or method, making references to the following simple NumPy and SciPy functions as needed:\n",
        "*   ndarray transpose https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html\n",
        "*ndarray shape https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html\n",
        "*   numpy hstack https://numpy.org/doc/stable/reference/generated/numpy.hstack.html\n",
        "*   numpy unique https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
        "*   numpy where https://numpy.org/doc/stable/reference/generated/numpy.where.html\n",
        "*numpy linspace https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
        "*numpy nan_to_num https://numpy.org/doc/stable/reference/generated/numpy.nan_to_num.html\n",
        "*numpy ones, zeros https://numpy.org/doc/stable/reference/generated/numpy.ones.html https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\n",
        "*  numpy unravel_index https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html\n",
        "*   collections.Counter and most_common() https://docs.python.org/2/library/collections.html\n",
        "*   scipy.stats.mode https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
        "  *   scipy ModeResult https://www.kite.com/python/docs/scipy.stats.mstats_basic.ModeResult\n",
        "*   scipy.stats.entropy https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIXeRDpbH87h"
      },
      "source": [
        "# Part A: preprocessing\n",
        "A function to preprocess our data matrix into something our decision tree can use\n",
        "\n",
        "Parameters:\n",
        "*   `data` is a numpy ndarray: the **full** data matrix with sample points as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `fill_mode` is a boolean (default True) for whether we want to fill in each missing value with the mode of its column\n",
        "*   `min_freq` is an integer (default 10): threshold for the minimum number of times a category must appear before we'd want to one-hot encode it into a binary variable\n",
        "*   `onehot_cols` is an array with the indeces of all the categorical variables in `data` to one-hot encode\n",
        "\n",
        "Fill in the `preprocess()` function as described:\n",
        "\n",
        "\n",
        "1.   Assign missing values (look for values that are `b''`) in the `data` table to `b'-1'` temporarily\n",
        "2.   Declare `onehot_encoding` and `onehot_features` to be empty arrays\n",
        "3.   Iterate through every `col` in `onehot_cols`\n",
        "\n",
        "  a. Pull column `col` from `data` (recall that `onehot_cols` is an array of column indices), and use it to intialize a `Counter()` called `counter`\n",
        "\n",
        "  b. Iterate through every `term` (`(category, count)` pair), in descending order by count (hint: check the `most_common()` method of `counter`)\n",
        "\n",
        "  *   Check if the `term`'s category represents a missing value (hint: what did you replace missing values with earlier?) if so, continue to next iteration\n",
        "  *   Check if the `term`'s count is below our threshold; if so, break the loop\n",
        "  *   Append `term`'s category to `onehot_features`, and a binary array to `onehot_encoding`, as described in the one-hot encoding process (hint: check `data`'s `col`-th column to get a boolean array, then cast it as an array of floats to get a binary array)\n",
        "\n",
        "  c. set `data`'s `col`-th column to `'0'` as it won't be used anymore\n",
        "4.   Convert `onehot_encoding` to a numpy array and transpose it; our binary arrays are now binary columns\n",
        "5.   Cast `data` as a numpy array of floats, then combine its columns with those of `onehot_encoding` (hint: check the `hstack()` function of numpy)\n",
        "6.   Check if `fill_mode` is enabled; if so, iterate through every column index `i` of `data`\n",
        "  \n",
        "  a.   Find the mode of column `i` for all non-missing values of the column; use the condition `(data[:, i] < -1 - eps) + (data[:, i] > -1 + eps)` to select rows of `data` where column `i` is **not** missing, then extract column `i` from the resulting table (hint: use `stats.mode()` from scipy, extract the mode from the ModeResult that it returns, and take element 0)\n",
        "  \n",
        "  b.   Assign the mode to **missing** values of column `i`; use a similar process as above to select missing values, but use the condition `(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)` instead\n",
        "7.   return `data, onehot_features`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqHwX_0RGpDJ"
      },
      "source": [
        "# Part B: DecisionTree constructor\n",
        "\n",
        "Creates a decision tree node\n",
        "\n",
        "Parameters:\n",
        "*   `max_depth` is an int (default 3) for how many levels of decision nodes we want in our current `DecisionTree` (a leaf would have `max_depth` 0)\n",
        "*   `feature_labels` is an array of strings (default None) for the names of our features\n",
        "\n",
        "Initialize the following based on the arguments passed in:\n",
        "*   `self.max_depth`\n",
        "*   `self.features`\n",
        "\n",
        "Initialize the following as an empty decision tree node:\n",
        "*   `self.left, self.right`\n",
        "*   `self.split_idx, self.thresh`\n",
        "*   `self.data, self.labels, self.pred`\n",
        "\n",
        "Note that the first four will be assigned in decision nodes, while the last three will be assigned in leaf nodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I29H2xyPSfNa"
      },
      "source": [
        "# Part C: entropy\n",
        "Calculates the entropy from a set of class labels\n",
        "\n",
        "Parameters:\n",
        "*   `labels` is a list of class labels (corresponding to points)\n",
        "*   `base` is an int (default None): the base to be used for the log function within entropy (None corresponds to natural log in `stats.entropy()`)\n",
        "\n",
        "Fill in the `entropy1()` function as described:\n",
        "1.   Get the counts for each class in `labels` (hint: check the `unique()` function of numpy)\n",
        "2.   Return the entropy calculated on the counts; you may use `stats.entropy() ` from scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kr6ot-PZBUm"
      },
      "source": [
        "# Part D: information gain\n",
        "Calculates the information gain from splitting points on feature column `X` and value `thresh`\n",
        "\n",
        "Parameters:\n",
        "*   `X` is an array of numeric values: the column of our data matrix that corresponds to the feature that we want to try splitting on\n",
        "*   `y` is an array of class labels (corresponding to points in data matrix)\n",
        "*   `thresh` is a numeric value: the value that we want to try splitting on\n",
        "\n",
        "Fill in the `information_gain()` function as described:\n",
        "1.   Find the two lists of **class labels** for the left and right children after the split, and create numpy arrays from these lists (hint: use list comprehensions with conditions `X[i] < thresh`, `X[i] >= thresh` as conditions)\n",
        "2.   Calculate the information gain from the left and right class labels like in **the warmup**, but with `entropy1()` in place of `entropy0()`\n",
        "3.   Return the information gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qaKRdqzOwqI"
      },
      "source": [
        "# Part E: split test\n",
        "Given a feature and split value, splits sample points and sample point indices in two\n",
        "\n",
        "Parameters:\n",
        "\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `idx` is an int: the column index of the feature that we want to split on\n",
        "*   `thresh` is a numeric value: the value that we want to split on\n",
        "\n",
        "Fill in the `split_test()` function as described:\n",
        "1.   Define `idx0`, `idx1` to be sets of sample point indices (rows in `X`) for the left and right children: ie. one where the values of feature `idx` are less than `thresh`, and another where they are greater than or equal to `thresh` (hint: check the `where()` function of numpy; provide a condition only and get the 0-th element of the function call)\n",
        "2.   Using the two sets of indices above, split `X` into two ndarrays `X0`, `X1`: one for the left child, and one for the right child\n",
        "3.   Return `X0`, `idx0`, `X1`, `idx1` in that order\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VADyeCCcT9Y-"
      },
      "source": [
        "# Part F: split\n",
        "Splits a set of data points **and** a set of class labels in two, given a feature and split value\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `y` is a list of class labels (corresponding to points)\n",
        "*   `idx` is an int: the column index of the feature that we want to split on\n",
        "*   `thresh` is a numeric value: the value that we want to split on\n",
        "\n",
        "Fill in the `split()` function as described:\n",
        "1.   Call `split_test()` to get `X0`, `idx0`, `X1`, `idx1` as described in the previous part\n",
        "2.   Use `idx0`, `idx1` to split `y` into two lists `y0` and `y1`\n",
        "3.   Return `X0`, `y0`, `X1`, `y1` in that order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBbRevyWEDeA"
      },
      "source": [
        "# Part G: fit\n",
        "Recursively grows a decision tree given a set of data points and a set of class labels\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "*   `y` is a list of class labels (corresponding to points)\n",
        "\n",
        "Fill in the `fit()` function as described:\n",
        "\n",
        "1.   Check if the `self.max_depth` of the tree is greater than 0\n",
        "\n",
        "Fill in the following under the main `if` clause\n",
        "2.   Create an empty array `gains` to store calculated information gains\n",
        "3.   Use list comprehension to create a 2D array: for each feature index `j`, use linear interpolation with `num=10` to find a 1D array of split values we'd like to try for that feature (hint #1: you can use `linspace()` in numpy to create the 1D array) (hint #2: use `X.shape[1]` to get the total number of features) \n",
        "\n",
        "*   Linear interpolation in this case: Take the minimum and maximum sample values of column `j`, x<sub>j-min</sub> and x<sub>j-max</sub>, and try `num` evenly spaced numbers over the interval [x<sub>j-min</sub> + eps, x<sub>j-max</sub> - eps] where eps is a very small number (hint #1: use `np.min()` and `np.max()`) (hint #2: `eps` is already defined)\n",
        "*   Finally, create a numpy array from the 2D array and assign it to `thresh_list`\n",
        "4.  For each feature index `j`:\n",
        "\n",
        "  a.  Use list comprehension to create a 1D array: for each value `t` in row `j` of `thresh_list`, call `information_gain()` on column `j` of `X`, `y`, and `t`\n",
        "\n",
        "  b.  Append the 1D array to `gains`\n",
        "5.  Convert `gains` to a numpy array, and call `np.nan_to_num()` on `gains` to replace NaN values with 0.0\n",
        "6.  Find the feature index and index of the threshold value that corresponds to the maximum information gain in `gains`, and assign them to `self.split_idx, thresh_idx`\n",
        "*   `gains` is a 2D array, but `np.argmax()` returns the 1D coordinate of the maximum gain along a **flattened** array! We need the 2D coordinates from `gain`, which can be found using `np.unravel_index(np.argmax(gains), gains.shape)`\n",
        "7.  Use `self.split_idx, thresh_idx` to find the threshold value in `thresh_list` that corresponds to the maximum gain; assign the value to `self.thresh` (hint: `thresh_list` is indexed the same way as `gains`)\n",
        "8.  Call `split()` to split our data point set and class label set in two: `X0, y0, X1, y1`\n",
        "9.  Using an `if` statement to check that both `X0` and `X1` are not empty: (this will dictate that the current node be a decision node)\n",
        "\n",
        "  a.  Construct two `DecisionTrees`, one for the left and one for the right child, and assign them to `self.left` and `self.right` (hint: what should you pass into the constructor for `max_depth` and `feature_labels`? How deep do you want a child to be in relation to `self.max_depth`?)\n",
        "\n",
        "  b.  Make a recursive call to `fit()`, for both `self.left` and `self.right`, passing in their respective data point sets and class label sets\n",
        "10.  Under `else`: (this will dictate a leaf node since either `X0`or `X1` is empty; we cannot split further)\n",
        "\n",
        "  a.  Set `self.max_depth` to 0\n",
        "\n",
        "  b.  Store `X, y` in `self.data, self.labels`\n",
        "\n",
        "  c.  Assign `self.pred`, the final prediction of the node, to the mode of `y` (hint: use `stats.mode()` from scipy, extract the mode from the ModeResult that it returns, and take element 0)\n",
        "\n",
        "Fill in the following under the main `else` clause (this will dictate a leaf node since we have reached the max depth)\n",
        "11.  As above, store `X, y` in `self.data, self.labels`, then assign `self.pred`, the final prediction of the node, to the mode of `y`\n",
        "\n",
        "Finally, outside of the `if-else` statements, return the current node `self`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQWUWgsFdfSD"
      },
      "source": [
        "# Part H: predict\n",
        "Recursively traverses a decision tree to predict class labels for a given set of data points\n",
        "\n",
        "Parameters:\n",
        "*   `X` is a numpy ndarray: the data matrix with sample points in the current node as rows, and predictive features (fields 2 through 10) as columns\n",
        "\n",
        "Fill in the `predict()` function as described:\n",
        "1.  Check if `self` is a leaf node (what would `max_depth` be?)\n",
        "  \n",
        "  a.  Return a 1D numpy array predicting `self.pred` for each sample point (hint #1: use the `*` operator to broadcast `self.pred` onto an array of ones the size of n, where n is the number of sample points) (hint #2: check `ones()` of numpy)\n",
        "2.  Else\n",
        "\n",
        "  a.  Call `split_test()` to split our data point set and index set in two: `X0, idx0, X1, idx1`\n",
        "\n",
        "  b.  Define `yhat` to be a 1D numpy array with a zero for each sample point (hint: check `zeros()` of numpy)\n",
        "\n",
        "  c.  Assign the values of `yhat` indexed by `idx0`, by recursively predicting on `X0` using the left child node\n",
        "  \n",
        "  d.  Repeat for `idx1` and `X1` using the right child node\n",
        "\n",
        "  e.  Return the node's predictions `yhat`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aie0fazLVYOq"
      },
      "source": [
        "class DecisionTree:\n",
        "    #Creates a decision tree node\n",
        "    def __init__(self, max_depth=3, feature_labels=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.features = feature_labels\n",
        "        # for non-leaf nodes\n",
        "        self.left, self.right = None, None\n",
        "        # for non-leaf nodes\n",
        "        self.split_idx, self.thresh = None, None\n",
        "        # for leaf nodes\n",
        "        self.data, self.labels, self.pred = None, None, None\n",
        "\n",
        "    #Calculates the entropy from a set of class labels\n",
        "    global entropy1\n",
        "    def entropy1(labels, base=None):\n",
        "        #get counts for each class label in labels; find and return entropy\n",
        "        value,counts = np.unique(labels, return_counts=True)\n",
        "        return stats.entropy(counts, base=base)\n",
        "    \n",
        "    @staticmethod\n",
        "    #Calculates the information gain from splitting points on feature column X and value thresh\n",
        "    def information_gain(X, y, thresh):\n",
        "        #condition X against thresh to split y into left and right child sets\n",
        "        left = np.array([y[i] for i in range(0, y.size) if X[i] < thresh])\n",
        "        right = np.array([y[i] for i in range(0, y.size) if X[i] >= thresh])\n",
        "        #calculate and return information gain from equation\n",
        "        gain = entropy1(y) - (left.size * entropy1(left) + right.size * entropy1(right)) / (left.size + right.size)\n",
        "        return gain\n",
        "\n",
        "    #Splits a set of data points and a set of class labels in two, given a feature and split value\n",
        "    def split(self, X, y, idx, thresh):\n",
        "        #call split_test, then use the two sets of indices to split y into two sets\n",
        "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
        "        y0, y1 = y[idx0], y[idx1]\n",
        "        #return X0, y0, X1, y1\n",
        "        return X0, y0, X1, y1\n",
        "\n",
        "    #Given a feature and split value, splits sample points and sample point indices in two\n",
        "    def split_test(self, X, idx, thresh):\n",
        "        #get two sets of sample point indices by conditioning column idx of X against thresh\n",
        "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
        "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
        "        #split X into two sets using the sample point indices above\n",
        "        #return X0, idx0, X1, idx1\n",
        "        X0, X1 = X[idx0, :], X[idx1, :]\n",
        "        return X0, idx0, X1, idx1\n",
        "\n",
        "    #Recursively grows a decision tree given a set of data points and a set of class labels\n",
        "    def fit(self, X, y):\n",
        "        #check self.max_depth\n",
        "        if self.max_depth > 0:\n",
        "            #create gains array\n",
        "            gains = []\n",
        "            #build thresh_list: values to try for each feature thresholding with a linear interpolation of 10 values; \n",
        "            #including logic to prevent thresholding on exactly the minimum or maximum values\n",
        "            #which may not lead to any meaningful node splits\n",
        "            thresh_list = np.array([\n",
        "                np.linspace(np.min(X[:, j]) + eps, np.max(X[:, j]) - eps, num=10)\n",
        "                for j in range(X.shape[1])\n",
        "            ])\n",
        "            #compute info gain for all single-dimension splits; convert to np array and remove NaNs\n",
        "            for j in range(X.shape[1]):\n",
        "                gains.append([self.information_gain(X[:, j], y, t) for t in thresh_list[j, :]])\n",
        "            gains = np.nan_to_num(np.array(gains))\n",
        "            #find optimal feature index and index of threshold value using argmax and unravel_index\n",
        "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
        "            #extract the optimal threshold value from thresh_list\n",
        "            self.thresh = thresh_list[self.split_idx, thresh_idx]\n",
        "            #call split() to get two sets of data pts and class labels\n",
        "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
        "            #check that neither X0 and X1 are empty (decision node): recursively build and fit left/right children\n",
        "            if X0.size > 0 and X1.size > 0:\n",
        "                self.left = DecisionTree(\n",
        "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
        "                self.left.fit(X0, y0)\n",
        "                self.right = DecisionTree(\n",
        "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
        "                self.right.fit(X1, y1)\n",
        "            #else (leaf node): set max_depth to 0, store X and y, and assign mode of y to be the final pred\n",
        "            else:\n",
        "                self.max_depth = 0\n",
        "                self.data, self.labels = X, y\n",
        "                self.pred = stats.mode(y).mode[0]\n",
        "        #else (leaf node): max_depth is 0, store X and y, and assign mode of y to be the final pred\n",
        "        else:\n",
        "            self.data, self.labels = X, y\n",
        "            self.pred = stats.mode(y).mode[0]\n",
        "        #return current node\n",
        "        return self\n",
        "\n",
        "    #Recursively traverses a decision tree to predict class labels for a given set of data points\n",
        "    def predict(self, X):\n",
        "        #check if leaf node: return pred for all sample pts\n",
        "        if self.max_depth == 0:\n",
        "            return self.pred * np.ones(X.shape[0])\n",
        "        #else (decision node): call split_test() to split into two sets of sample pts and sample pt indices;\n",
        "        #create yhat = array of 0's;\n",
        "        #recursively predict on the two sets using the left and right children and insert into yhat\n",
        "        else:\n",
        "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
        "            yhat = np.zeros(X.shape[0])\n",
        "            yhat[idx0] = self.left.predict(X0)\n",
        "            yhat[idx1] = self.right.predict(X1)\n",
        "            #return all predictions\n",
        "            return yhat\n",
        "        \n",
        "#A function to preprocess our data matrix into something our decision tree can use\n",
        "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
        "    # Temporarily assign -1 to missing data\n",
        "    data[data == b''] = b'-1'\n",
        "\n",
        "    # Hash the columns (used for handling strings)\n",
        "    onehot_encoding = []\n",
        "    onehot_features = []\n",
        "    for col in onehot_cols:\n",
        "        #initialize a Counter from data column, to find the most common categorical terms and their frequencies\n",
        "        counter = Counter(data[:, col])\n",
        "        for term in counter.most_common():\n",
        "            #ignore missing terms and only encode category if frequency is above threshold\n",
        "            if term[0] == b'-1':\n",
        "                continue\n",
        "            if term[-1] <= min_freq:\n",
        "                break\n",
        "            #keep track of feature-column correspondence, then one-hot encode into binary arrays\n",
        "            #neutralize original column\n",
        "            #transpose to get binary columns and add them to data matrix\n",
        "            onehot_features.append(term[0])\n",
        "            onehot_encoding.append((data[:, col] == term[0]).astype(np.float))\n",
        "        data[:, col] = '0'\n",
        "    onehot_encoding = np.array(onehot_encoding).T\n",
        "    data = np.hstack([np.array(data, dtype=np.float), np.array(onehot_encoding)])\n",
        "\n",
        "    # Replace missing data with the mode value. We use the mode instead of\n",
        "    # the mean or median because this makes more sense for categorical\n",
        "    # features such as gender or cabin type, which are not ordered.\n",
        "    if fill_mode:\n",
        "        for i in range(data.shape[-1]):\n",
        "            #get values of column i that are not close to -1\n",
        "            #get the mode of the above using scipy.stats.mode and accessing ModeResult\n",
        "            #assign mode to values of column i that are close to -1\n",
        "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
        "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
        "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
        "    #return data, onehot_features\n",
        "    return data, onehot_features\n",
        "\n",
        "\n",
        "def evaluate(clf):\n",
        "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
        "    if hasattr(clf, \"decision_trees\"):\n",
        "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
        "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
        "        print(\"First splits\", first_splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef-4GLLdd_La"
      },
      "source": [
        "# Preprocessing\n",
        "After implementing all the decision tree functions, run the cell below to preprocess our Titanic training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1joGH4W0VZMW",
        "outputId": "59e214c5-5abf-4ee4-c257-684dd8c313ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#import titanic dataset files as training and test data\n",
        "path_train = '/content/drive/My Drive/Colab Notebooks/decision trees/titanic_training.csv'\n",
        "data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
        "\n",
        "path_test = '/content/drive/My Drive/Colab Notebooks/decision trees/titanic_testing_data.csv'\n",
        "test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
        "\n",
        "#define our set of class labels from imported test data (0 = Died, 1 = Survived)\n",
        "y_titanic = data[1:, 0]\n",
        "class_names_titanic = [\"Died\", \"Survived\"]\n",
        "\n",
        "#for class labels AND preprocessed data, use labeled_idx to keep only rows where class labels are present\n",
        "labeled_idx = np.where(y_titanic != b'')[0]\n",
        "y_titanic = np.array(y_titanic[labeled_idx], dtype=np.int)\n",
        "\n",
        "#preprocess training data and test data\n",
        "print(\"Preprocessing the titanic dataset\")\n",
        "X_titanic, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
        "X_titanic = X_titanic[labeled_idx, :]\n",
        "Z_titanic, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
        "\n",
        "#check that our two sets have same # of columns\n",
        "#define list of feature names\n",
        "assert X_titanic.shape[1] == Z_titanic.shape[1]\n",
        "features_titanic = list(data[0, 1:]) + onehot_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing the titanic dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmLQoG9f83HW"
      },
      "source": [
        "# Before using our `DecisionTree`:\n",
        "\n",
        "\n",
        "*   Split our training data into training and validation sets\n",
        "  *  Randomly assign 20% of data to validation set, keeping 80% in training set; this is commonly called the \"80/20\" split\n",
        "*  Print the names of our features, before and after the preprocessing of our data matrices\n",
        "  *  Recall that we one-hot encoded while preprocessing!\n",
        "*  Run a \"dummy\" constant classifier on training set, which predicts 0 (\"didn't survive\") for each passenger\n",
        "  *  We'll refer back to this as a baseline accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IKPNLHXVgco",
        "outputId": "7760bba8-d8b0-4264-942c-30cf640ed085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#split training into 80% training, 20% validation; use random state to keep it deterministic for our purposes\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=49)\n",
        "\n",
        "#print train, test, train/val sizes\n",
        "print(\"Total training size:\", X_titanic.shape[0])\n",
        "print(\"Total test size:\", Z_titanic.shape[0])\n",
        "print(\"Training/validation sizes:\", X_train.shape[0], X_val.shape[0])\n",
        "\n",
        "#print our feature labels before and after one-hot encoding\n",
        "print(\"\\n\\nFeatures before one-hot encoding:\", list(data[0, 1:]))\n",
        "print(\"Features after one-hot encoding:\", features_titanic)\n",
        "\n",
        "#constant classifier which always predicts 0\n",
        "print(\"\\n\\nAccuracy of constant classifier:\", 1 - np.sum(y_titanic) / y_titanic.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training size: 999\n",
            "Total test size: 310\n",
            "Training/validation sizes: 799 200\n",
            "\n",
            "\n",
            "Features before one-hot encoding: [b'pclass', b'sex', b'age', b'sibsp', b'parch', b'ticket', b'fare', b'cabin', b'embarked']\n",
            "Features after one-hot encoding: [b'pclass', b'sex', b'age', b'sibsp', b'parch', b'ticket', b'fare', b'cabin', b'embarked', b'male', b'female', b'S', b'C', b'Q']\n",
            "\n",
            "\n",
            "Accuracy of constant classifier: 0.6136136136136137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWFtJXSfLUQC"
      },
      "source": [
        "# Training our `DecisionTree`\n",
        "We're finally ready to apply our `DecisionTree` to the training set! Run the cell below to:\n",
        "\n",
        "\n",
        "*   Create an instance of `DecisionTree` and grow it on our set of training points\n",
        "*  Use the `DecisionTree` to predict on the training and validation sets, and obtain the training and validation accuracies\n",
        "  *  The training accuracy is how well our decision tree is able to model the dataset that it was constructed on\n",
        "  *  The validation accuracy is how well it can generalize to predict on data that it hasn't seen yet\n",
        "*  Check that training and validation accuracies match the staff accuracies (which should be the case if `DecisionTree` was implemented as described)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88LZvgU5Gs47",
        "outputId": "1867cfdf-295a-4a47-c31e-13b41d4c0ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#create a DecisionTree instance and train it on the training set\n",
        "dt = DecisionTree(max_depth=8, feature_labels=features_titanic)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "#use it to predict on our training set and validation set\n",
        "train_pred = dt.predict(X_train)\n",
        "val_pred = dt.predict(X_val)\n",
        "train_acc = 1 - np.count_nonzero(train_pred - y_train) / y_train.size\n",
        "val_acc = 1 - np.count_nonzero(val_pred - y_val) / y_val.size\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=2)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(train_acc)\n",
        "  0.8660826032540676\n",
        "  >>> print(val_acc)\n",
        "  0.795\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.8660826032540676\n",
            "Validation Accuracy: 0.795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_FfK9tQ6Ia"
      },
      "source": [
        "# Final Predictions\n",
        "Once we are satisfied with our validation accuracy, we can make our final predictions on the test set\n",
        "\n",
        "*   Now that validation is complete, we should train on our **entire** training set (including points previously assigned to validation) before making test predictions\n",
        "*   Predict on the test set, and ensure that the test predictions match the staff predictions\n",
        "  *  Recall that we don't have the true class labels for the test set, so we cannot obtain a test accuracy! However, a test accuracy would be used in practice as a final evaluation of the model's performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsP4J_I9GwHR",
        "outputId": "18a2bc9a-2e63-4880-c094-4c6347b7b923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#train on entire training set\n",
        "dt.fit(X_titanic, y_titanic)\n",
        "test_pred = dt.predict(Z_titanic)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "#Test with the following doctest test vectors.\n",
        "#DO NOT EDIT THE TEST CODE!!!!\n",
        "#Even changing the spacing can cause errors.\n",
        "#The test code will automatically execute when you run the cell.\n",
        "#You should test all your combination of outputs but your code at least must pass these exact tests.\n",
        "#If your code fails, you will see a description in the console cell.\n",
        "#If your code passes, you will see the message: \"TestResults(failed=0, attempted=1)\"\n",
        "import doctest\n",
        "\"\"\"\n",
        "  >>> print(test_pred)\n",
        "  [1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
        "   0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
        "   1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
        "   1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n",
        "   1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
        "   0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
        "   0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
        "   0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
        "   1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
        "   0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
        "\"\"\"\n",
        "doctest.testmod()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVVO7bxbaT5l"
      },
      "source": [
        "# Bonus: Visualization\n",
        "\n",
        "Run the following code to visualize the decision tree trained above— but limited to depth 3, for visibility\n",
        "\n",
        "*   The PDF file will show up in `/Colab Notebooks/decision trees/test-output/`\n",
        "*  Each decision node shows the name of the feature (corresponding to `self.split_idx`), and the threshold value the feature is split on `self.thresh`\n",
        "*  Each leaf node shows its final prediction `self.pred`\n",
        "*  It should look like: ![](https://drive.google.com/uc?export=view&id=1PE9UNo-df4d7KGacACFaiKOLWT9hOGBK)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4TXyyDwacJM",
        "outputId": "baf16b19-ddf4-4513-c18f-6e68ed8733e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!{sys.executable} -m pip install graphviz\n",
        "from graphviz import Digraph\n",
        "import os\n",
        "\n",
        "tree_vis = Digraph(comment='Titanic decision tree')\n",
        "i = 0\n",
        "\n",
        "#recursive function that walks through a tree and builds the Digraph node by node (either threshold or class label)\n",
        "def walk(DT):\n",
        "    global i\n",
        "    #leaf node\n",
        "    if DT.max_depth == 0:\n",
        "        #nodenum = unique label for every node\n",
        "        nodenum = str(i)\n",
        "        tree_vis.node(nodenum, 'class: ' + str(DT.pred))\n",
        "        i += 1\n",
        "        return nodenum\n",
        "    #decision node; recursive calls\n",
        "    else:\n",
        "        #print feature to split on, and threshold\n",
        "        label = str(DT.features[DT.split_idx]) + ', threshold ' + str(DT.thresh)\n",
        "        nodenum = str(i)\n",
        "        i += 1\n",
        "        tree_vis.node(nodenum, label)\n",
        "        tree_vis.edge(nodenum, walk(DT.left))\n",
        "        tree_vis.edge(nodenum, walk(DT.right))\n",
        "        return nodenum\n",
        "\n",
        "#create and train the depth 3 DecisionTree\n",
        "dt3 = DecisionTree(max_depth=3, feature_labels=features_titanic)\n",
        "dt3.fit(X_train, y_train)\n",
        "\n",
        "walk(dt3)\n",
        "#un-comment to print the link between nodes\n",
        "#print(tree_vis.source)\n",
        "\n",
        "#delete existing output graph if exists\n",
        "#render graph\n",
        "try:\n",
        "    os.remove('test-output/titanic_3.gv')\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "tree_vis.render('/content/drive/My Drive/Colab Notebooks/decision trees/test-output/titanic_3.gv', view=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/decision trees/test-output/titanic_3.gv.pdf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}